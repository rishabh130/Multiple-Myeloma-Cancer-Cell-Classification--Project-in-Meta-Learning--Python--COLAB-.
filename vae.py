import torch
from torch import nn
from torch.nn import functional as F
import math
from torchvision import datasets, models, transforms
# from sd_layer_pytorch_mod import *


class classifier(nn.Module):

    def __init__(self):
        super(classifier, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn7 = nn.BatchNorm2d(256)
        self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn9 = nn.BatchNorm2d(512)
        self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(512, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*12*12, 256)
        self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        x = F.relu(self.bn4(self.conv4(x)))
        x = F.relu(self.bn5(self.conv5(x)))
        x = F.relu(self.bn6(self.conv6(x)))
        x = F.relu(self.bn7(self.conv7(x)))
        x = F.relu(self.bn8(self.conv8(x)))
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        x = x.view(-1, 512*12*12)
        x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc2(x))
        return x



class classifier_new(nn.Module):

    def __init__(self):
        super(classifier_new, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 32, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn7 = nn.BatchNorm2d(256)
        self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn9 = nn.BatchNorm2d(512)
        self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(512, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*19*19, 256)
        self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =self.drop1( F.relu(self.bn1(self.conv1(x))))
        x =self.drop2( F.relu(self.bn2(self.conv2(x))))
        x =self.drop3( F.relu(self.bn3(self.conv3(x))))
        x =self.drop4( F.relu(self.bn4(self.conv4(x))))
        x =self.drop5( F.relu(self.bn5(self.conv5(x))))
        x =self.drop6( F.relu(self.bn6(self.conv6(x))))
        x =self.drop7( F.relu(self.bn7(self.conv7(x))))
        x =self.drop8( F.relu(self.bn8(self.conv8(x))))
        x =self.drop9( F.relu(self.bn9(self.conv9(x))))
        x =self.drop10( F.relu(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*19*19)
        x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc2(x), dim=1)
        return x

class classifier_new_with_gap(nn.Module):

    def __init__(self):
        super(classifier_new_with_gap, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 32, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn7 = nn.BatchNorm2d(256)
        self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn9 = nn.BatchNorm2d(512)
        self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(512, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.drop10=nn.Dropout2d(p=prob)
        self.gap = nn.AvgPool2d(5)
        self.fc1 = nn.Linear(512, 2)
        # self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =self.drop1( F.relu(self.bn1(self.conv1(x))))
        x =self.drop2( F.relu(self.bn2(self.conv2(x))))
        x =self.drop3( F.relu(self.bn3(self.conv3(x))))
        x =self.drop4( F.relu(self.bn4(self.conv4(x))))
        x =self.drop5( F.relu(self.bn5(self.conv5(x))))
        x =self.drop6( F.relu(self.bn6(self.conv6(x))))
        x =self.drop7( F.relu(self.bn7(self.conv7(x))))
        x =self.drop8( F.relu(self.bn8(self.conv8(x))))
        x =self.drop9( F.relu(self.bn9(self.conv9(x))))
        x =self.drop10( F.relu(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        x = self.gap(x)
        # print(x.shape)
        x = x.view(-1, 512)
        x=F.relu(self.fc1(x))
        x = F.log_softmax(x)
        return x

class classifier_new_with_prelu(nn.Module):

    def __init__(self):
        super(classifier_new_with_prelu, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1=nn.Dropout2d(p=prob)
        self.prelu1=nn.PReLU()
        self.conv2 = nn.Conv2d(32, 32, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.drop2=nn.Dropout2d(p=prob)
        self.prelu2=nn.PReLU()
        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.drop3=nn.Dropout2d(p=prob)
        self.prelu3=nn.PReLU()
        self.conv4 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4=nn.Dropout2d(p=prob)
        self.prelu4=nn.PReLU()
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.drop5=nn.Dropout2d(p=prob)
        self.prelu5=nn.PReLU()
        self.conv6 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.drop6=nn.Dropout2d(p=prob)
        self.prelu6=nn.PReLU()
        self.conv7 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn7 = nn.BatchNorm2d(256)
        self.drop7=nn.Dropout2d(p=prob)
        self.prelu7=nn.PReLU()
        self.conv8 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.drop8=nn.Dropout2d(p=prob)
        self.prelu8=nn.PReLU()
        self.conv9 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn9 = nn.BatchNorm2d(512)
        self.drop9=nn.Dropout2d(p=prob)
        self.prelu9=nn.PReLU()
        self.conv10 = nn.Conv2d(512, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.drop10=nn.Dropout2d(p=prob)
        self.prelu10=nn.PReLU()
        self.gap = nn.AvgPool2d(19)
        self.fc1 = nn.Linear(512, 2)
        # self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =self.drop1( self.prelu1(self.bn1(self.conv1(x))))
        x =self.drop2( self.prelu2(self.bn2(self.conv2(x))))
        x =self.drop3( self.prelu3(self.bn3(self.conv3(x))))
        x =self.drop4( self.prelu4(self.bn4(self.conv4(x))))
        x =self.drop5( self.prelu5(self.bn5(self.conv5(x))))
        x =self.drop6( self.prelu6(self.bn6(self.conv6(x))))
        x =self.drop7( self.prelu7(self.bn7(self.conv7(x))))
        x =self.drop8( self.prelu8(self.bn8(self.conv8(x))))
        x =self.drop9( self.prelu9(self.bn9(self.conv9(x))))
        x =self.drop10( self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        x = self.gap(x)
        # print(x.shape)
        x = x.view(-1, 512)
        x=F.relu(self.fc1(x))
        x = F.log_softmax(x, dim=1)
        return x


class classifier_deeper_with_prelu(nn.Module):

    def __init__(self):
        super(classifier_deeper_with_prelu, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1=nn.Dropout2d(p=prob)
        self.prelu1=nn.PReLU()
        self.conv2 = nn.Conv2d(32, 32, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.drop2=nn.Dropout2d(p=prob)
        self.prelu2=nn.PReLU()
        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.drop3=nn.Dropout2d(p=prob)
        self.prelu3=nn.PReLU()
        self.conv4 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn4 = nn.BatchNorm2d(128)
        self.drop4=nn.Dropout2d(p=prob)
        self.prelu4=nn.PReLU()
        self.conv5 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.drop5=nn.Dropout2d(p=prob)
        self.prelu5=nn.PReLU()
        self.conv6 = nn.Conv2d(128, 256, 3, stride=2)
        self.bn6 = nn.BatchNorm2d(256)
        self.drop6=nn.Dropout2d(p=prob)
        self.prelu6=nn.PReLU()
        self.conv7 = nn.Conv2d(256, 512, 3, stride=1)
        self.bn7 = nn.BatchNorm2d(512)
        self.drop7=nn.Dropout2d(p=prob)
        self.prelu7=nn.PReLU()
        self.conv8 = nn.Conv2d(512, 512, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(512)
        self.drop8=nn.Dropout2d(p=prob)
        self.prelu8=nn.PReLU()
        self.conv9 = nn.Conv2d(512, 1024, 3, stride=2)
        self.bn9 = nn.BatchNorm2d(1024)
        self.drop9=nn.Dropout2d(p=prob)
        self.prelu9=nn.PReLU()
        self.conv10 = nn.Conv2d(1024, 1024, 3, stride=1)
        self.bn10 = nn.BatchNorm2d(1024)
        self.drop10=nn.Dropout2d(p=prob)
        self.prelu10=nn.PReLU()
        self.conv11 = nn.Conv2d(1024, 1024, 3, stride=2)
        self.bn11 = nn.BatchNorm2d(1024)
        self.drop11=nn.Dropout2d(p=prob)
        self.prelu11=nn.PReLU()
        self.gap = nn.AvgPool2d(4)
        self.fc1 = nn.Linear(1024, 2)
        # self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =self.drop1( self.prelu1(self.bn1(self.conv1(x))))
        x =self.drop2( self.prelu2(self.bn2(self.conv2(x))))
        x =self.drop3( self.prelu3(self.bn3(self.conv3(x))))
        x =self.drop4( self.prelu4(self.bn4(self.conv4(x))))
        x =self.drop5( self.prelu5(self.bn5(self.conv5(x))))
        x =self.drop6( self.prelu6(self.bn6(self.conv6(x))))
        x =self.drop7( self.prelu7(self.bn7(self.conv7(x))))
        x =self.drop8( self.prelu8(self.bn8(self.conv8(x))))
        x =self.drop9( self.prelu9(self.bn9(self.conv9(x))))
        x =self.drop10( self.prelu10(self.bn10(self.conv10(x))))
        x =self.drop11( self.prelu11(self.bn11(self.conv11(x))))
        # print(x.shape)
        #print(x.shape)
        # print(x.shape)
        x = self.gap(x)
        # print(x.shape)
        x = x.view(-1, 1024)
        x=F.relu(self.fc1(x))
        x = F.log_softmax(x)
        return x

class classifier_mm(nn.Module):

    def __init__(self):
        super(classifier_mm, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =( F.relu(self.bn1(self.conv1(x))))
        x =( F.relu(self.bn2(self.conv2(x))))
        x =( F.relu(self.bn3(self.conv3(x))))
        x =( F.relu(self.bn4(self.conv4(x))))
        x =( F.relu(self.bn5(self.conv5(x))))
        x =( F.relu(self.bn6(self.conv6(x))))
        x =( F.relu(self.bn7(self.conv7(x))))
        x =( F.relu(self.bn8(self.conv8(x))))
        x =( F.relu(self.bn9(self.conv9(x))))
        x =( F.relu(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu_gap(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_gap, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.gap=nn.AvgPool2d(6)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        x=self.gap(x)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x


class classifier_mm_prelu_with_fc(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_with_fc, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 64)
        self.prelu11=nn.PReLU()
        self.fc2 = nn.Linear(64, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        x=self.prelu11(self.fc1(x))
        x = F.log_softmax(self.fc2(x), dim=1)
        return x

class classifier_mm_prelu_deeper(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_deeper, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2, dilation=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()

        self.conv11 = nn.Conv2d(512, 512, 3, stride=1, dilation=2, padding=1)
        self.bn11 = nn.BatchNorm2d(512)
        self.prelu11=nn.PReLU()

        self.conv12 = nn.Conv2d(512, 512, 3, stride=1, dilation=2, padding=1)
        self.bn12 = nn.BatchNorm2d(512)
        self.prelu12=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*1*1, 2)
        # self.prelu13=nn.PReLU()
        # self.fc2 = nn.Linear(64, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        #print(x.shape)
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        x =(self.prelu11(self.bn11(self.conv11(x))))
        #print(x.shape)
        x =(self.prelu12(self.bn12(self.conv12(x))))
        #print(x.shape)
        #print(x.shape)
        #print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*1*1)
        # x=self.prelu13(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu_deeper_with_fc(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_deeper_with_fc, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2, dilation=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()

        self.conv11 = nn.Conv2d(512, 512, 3, stride=1, dilation=2, padding=1)
        self.bn11 = nn.BatchNorm2d(512)
        self.prelu11=nn.PReLU()

        self.conv12 = nn.Conv2d(512, 512, 3, stride=1, dilation=2, padding=1)
        self.bn12 = nn.BatchNorm2d(512)
        self.prelu12=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*1*1, 64)
        self.prelu13=nn.PReLU()
        self.fc2 = nn.Linear(64, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        # print(x.shape)
        x =(self.prelu10(self.bn10(self.conv10(x))))
        # print(x.shape)
        x =(self.prelu11(self.bn11(self.conv11(x))))
        # print(x.shape)
        x =(self.prelu12(self.bn12(self.conv12(x))))
        #print(x.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*1*1)
        x=self.prelu13(self.fc1(x))
        x = F.log_softmax(self.fc2(x), dim=1)
        return x


class classifier_mm_lrelu(nn.Module):

    def __init__(self):
        super(classifier_mm_lrelu, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.LeakyReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.LeakyReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.LeakyReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.LeakyReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.LeakyReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.LeakyReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.LeakyReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.LeakyReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.LeakyReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.LeakyReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu_sd(nn.Module):

    def __init__(self, ref_image):
        super(classifier_mm_prelu_sd, self).__init__()
        prob=.5
        self.sd_layer=sd_layer_pytorch(ref_image)
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x=self.sd_layer(x)
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu_sd_dct(nn.Module):

    def __init__(self, ref_image, gpu_no):
        super(classifier_mm_prelu_sd_dct, self).__init__()
        prob=.5
        self.sd_layer=sd_layer_pytorch_modular_dct_no_threshold_trainable(ref_image, gpu_no)
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x=self.sd_layer(x)
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = F.log_softmax(self.fc1(x), dim=1)
        return x

class classifier_mm_prelu_without_logsoftmax(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_without_logsoftmax, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)
        #x=F.relu(self.fc1(x))
        x = self.fc1(x)
        return x

class classifier_mm_prelu_prototype(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        self.dce=dce_loss(2,512*6*6)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)

        #x=F.relu(self.fc1(x))
        x1 = self.prelu11((x))
        centers,x=self.dce(x1)
        output = F.log_softmax(x, dim=1)
        return x1,centers,x,output

class classifier_mm_prelu_prototype_bce(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_bce, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        self.prelu12=nn.PReLU()
        self.dce=my_loss(2,512)
        self.gap=nn.AvgPool2d(6)
        self.gap_1=nn.Linear(512,64)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=self.gap(x)
        # print(x_gap.shape)
        x_gap=(x_gap.view(-1, 512))
        # x_gap=(self.gap_1(x_gap))

        # print(x_gap.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)

        x_bce=(self.fc1(x))
        x_bce=F.log_softmax(x_bce, dim=1)

        x1 = self.prelu11((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(2*x_c, dim=1)
        return x1,centers,x_c,output, x_bce

class classifier_mm_prelu_prototype_bce_visualization(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_bce_visualization, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        self.prelu12=nn.PReLU()
        self.prelu13=nn.PReLU()
        self.dce=dce_loss(2,2)
        self.gap=nn.AvgPool2d(6)
        self.gap_1=nn.Linear(512,2)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        self.fc2 = nn.Linear(2, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=self.gap(x)
        # print(x_gap.shape)
        x_gap=self.prelu13(x_gap.view(-1, 512))
        x_gap=(self.gap_1(x_gap))

        # print(x_gap.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)

        x=self.prelu12(self.fc1(x))
        x_bce=self.fc2(x)

        x_bce=F.log_softmax(x_bce, dim=1)

        x1 = self.prelu11((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(2*x_c, dim=1)
        return x1,centers,x_c,output, x_bce
class classifier_mm_prelu_prototype_my_loss_visualization(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_my_loss_visualization, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        self.prelu12=nn.PReLU()
        self.prelu13=nn.PReLU()
        self.dce=dce_loss(2,2)
        self.gap=nn.AvgPool2d(6)
        self.gap_1=nn.Linear(512,2)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        self.fc2 = nn.Linear(2, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=self.gap(x)
        # print(x_gap.shape)
        x_gap=self.prelu13(x_gap.view(-1, 512))
        x_gap=(self.gap_1(x_gap))

     
        

        x1 = self.prelu11((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(2*x_c, dim=1)
        return x1,centers,x_c,output
        
def new_regularization(centers, labels):

        centers_copy=centers.clone()
        batch_size=centers.shape[0]
        centers_copy[torch.arange(batch_size), labels]=1
        centers_copy=torch.where(centers_copy==1, centers_copy, torch.zeros(batch_size,2).cuda())
        centers_copy_new=2*centers_copy-1
        centers=centers*centers_copy
        dist_max, dist_max_idx=torch.max(centers,0, keepdim=True)
        centers=(centers-dist_max)*centers_copy_new
        dist_min, dist_min_idx=torch.min(centers,0, keepdim=True)
        dist_min=dist_min+dist_max
        dist_diff=torch.pow((centers-dist_min)*centers_copy,2)
        dist_diff=torch.sum(dist_diff,0,keepdim=True)
        dist_diff=torch.sum(dist_diff,1,keepdim=True)

        return dist_diff/batch_size

class classifier_mm_prelu_prototype_bce_normalized(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_bce_normalized, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=2)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        self.drop9=nn.Dropout2d(p=prob)

        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()

        self.conv11 = nn.Conv2d(512, 512, 3, stride=1)
        self.bn11 = nn.BatchNorm2d(512)
        self.prelu11=nn.PReLU()

        self.conv12 = nn.Conv2d(512, 1024, 3, stride=2)
        self.bn12 = nn.BatchNorm2d(1024)
        self.prelu12=nn.PReLU()


        self.prelu13=nn.PReLU()
        # self.prelu12=nn.PReLU()
        self.dce=my_loss(2,256)
        self.gap=nn.AvgPool2d(8)
        self.gap_1=nn.Linear(256,2)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(256*8*8, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        # x =(self.prelu9(self.bn9(self.conv9(x))))
        # x =(self.prelu10(self.bn10(self.conv10(x))))
        # x =(self.prelu11(self.bn11(self.conv11(x))))
        # x =(self.prelu12(self.bn12(self.conv12(x))))
        # print(x.shape)
        # x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=self.gap(x)
        # print(x_gap.shape)
        x_gap=x_gap.view(-1, 256)
        # x_gap=(self.gap_1(x_gap))

        # print(x_gap.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 256*8*8)

        x_bce=(self.fc1(x))
        x_bce=F.log_softmax(x_bce, dim=1)

        x1 = self.prelu13((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(2*x_c, dim=1)
        return x1,centers,x_c,output, x_bce

class classifier_mm_prelu_prototype_bce_without_gap(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_bce_without_gap, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        self.prelu11=nn.PReLU()
        # self.prelu12=nn.PReLU()
        self.dce=my_loss(2,1024)
        self.gap=nn.AvgPool2d(6)
        self.gap_1=nn.Linear(512,2)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        self.fc11 = nn.Linear(512*6*6, 1024)
        self.fc2 = nn.Linear(1024, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=x.view(-1, 512*6*6)
        x_gap=(self.fc11(x_gap))
        # x_gap=self.fc2(x_gap)

        # print(x_gap.shape)
        # x_gap=x_gap.view(-1, 512)
        # x_gap=(self.gap_1(x_gap))

        # print(x_gap.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)

        x_bce=(self.fc1(x))
        x_bce=F.log_softmax(x_bce, dim=1)

        x1 = self.prelu11((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(x_c, dim=1)
        return x1,centers,x_c,output, x_bce



class classifier_mm_prelu_prototype_bce_v2(nn.Module):

    def __init__(self):
        super(classifier_mm_prelu_prototype_bce_v2, self).__init__()
        prob=.5
        self.conv1 = nn.Conv2d(3, 32, 3, stride=4)
        self.bn1 = nn.BatchNorm2d(32)
        self.prelu1=nn.PReLU()
        #self.drop1=nn.Dropout2d(p=prob)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.prelu2=nn.PReLU()
        #self.drop2=nn.Dropout2d(p=prob)
        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.prelu3=nn.PReLU()
        #self.drop3=nn.Dropout2d(p=prob)
        self.conv4 = nn.Conv2d(64, 64, 3, stride=2)
        self.bn4 = nn.BatchNorm2d(64)
        self.prelu4=nn.PReLU()
        #self.drop4=nn.Dropout2d(p=prob)
        self.conv5 = nn.Conv2d(64, 128, 3, stride=1)
        self.bn5 = nn.BatchNorm2d(128)
        self.prelu5=nn.PReLU()
        #self.drop5=nn.Dropout2d(p=prob)
        self.conv6 = nn.Conv2d(128, 128, 3, stride=1)
        self.bn6 = nn.BatchNorm2d(128)
        self.prelu6=nn.PReLU()
        #self.drop6=nn.Dropout2d(p=prob)
        self.conv7 = nn.Conv2d(128, 128, 3, stride=2)
        self.bn7 = nn.BatchNorm2d(128)
        self.prelu7=nn.PReLU()
        #self.drop7=nn.Dropout2d(p=prob)
        self.conv8 = nn.Conv2d(128, 256, 3, stride=1)
        self.bn8 = nn.BatchNorm2d(256)
        self.prelu8=nn.PReLU()
        #self.drop8=nn.Dropout2d(p=prob)
        self.conv9 = nn.Conv2d(256, 256, 3, stride=1)
        self.bn9 = nn.BatchNorm2d(256)
        self.prelu9=nn.PReLU()
        #self.drop9=nn.Dropout2d(p=prob)
        self.conv10 = nn.Conv2d(256, 512, 3, stride=2)
        self.bn10 = nn.BatchNorm2d(512)
        self.prelu10=nn.PReLU()
        self.prelu11=nn.PReLU()
        # self.prelu12=nn.PReLU()
        self.dce=my_loss(2,512)
        self.gap=nn.AvgPool2d(6)
        self.gap_1=nn.Linear(512,2)
        #self.drop10=nn.Dropout2d(p=prob)
        # self.gap = nn.AvgPool2d(12)
        self.fc1 = nn.Linear(512*6*6, 2)
        #self.fc2 = nn.Linear(256, 2)
        # self.logsoft= F.LogSoftmax()


    def forward(self, x):
        x =(self.prelu1(self.bn1(self.conv1(x))))
        x =(self.prelu2(self.bn2(self.conv2(x))))
        x =(self.prelu3(self.bn3(self.conv3(x))))
        x =(self.prelu4(self.bn4(self.conv4(x))))
        x =(self.prelu5(self.bn5(self.conv5(x))))
        x =(self.prelu6(self.bn6(self.conv6(x))))
        x =(self.prelu7(self.bn7(self.conv7(x))))
        x =(self.prelu8(self.bn8(self.conv8(x))))
        x =(self.prelu9(self.bn9(self.conv9(x))))
        x =(self.prelu10(self.bn10(self.conv10(x))))

        x_gap=self.gap(x)
        # print(x_gap.shape)
        x_gap=(x_gap.view(-1, 512))
        # x_gap=(self.gap_1(x_gap))

        # print(x_gap.shape)
        #print(x.shape)
        # print(x.shape)
        # x = self.gap(x)
        # print(x.shape)
        # print(x.shape)
        x = x.view(-1, 512*6*6)

        x_bce=(self.fc1(x))
        x_bce=F.log_softmax(x_bce, dim=1)

        x1 = self.prelu11((x_gap))
        centers,x_c=self.dce(x1)
        output = F.log_softmax(2*x_c, dim=1)
        return x1,centers,x_c,output, x_bce


class dce_loss(torch.nn.Module):
    def __init__(self, n_classes,feat_dim,init_weight=True):

        super(dce_loss, self).__init__()
        self.n_classes=n_classes
        self.feat_dim=feat_dim
        self.centers=nn.Parameter(torch.randn(self.feat_dim,self.n_classes).cuda(),requires_grad=True)
        if init_weight:
            self.__init_weight()

    def __init_weight(self):
        nn.init.kaiming_normal_(self.centers)



    def forward(self, x):

        features_square=torch.sum(torch.pow(x,2),1, keepdim=True)
        centers_square=torch.sum(torch.pow(self.centers,2),0, keepdim=True)
        features_into_centers=2*torch.matmul(x, (self.centers))
        dist=features_square+centers_square-features_into_centers

        return self.centers, -dist

def regularization(features, centers, labels):
        distance=(features-torch.t(centers)[labels])

        distance=torch.sum(torch.pow(distance,2),1, keepdim=True)

        distance=(torch.sum(distance, 0, keepdim=True))/features.shape[0]

        return distance

class my_loss(torch.nn.Module):
    def __init__(self, n_classes,feat_dim,init_weight=True):

        super(my_loss, self).__init__()
        self.n_classes=n_classes
        self.feat_dim=feat_dim
        self.centers=nn.Parameter(torch.randn(self.feat_dim,self.n_classes).cuda(),requires_grad=True)
        if init_weight:
            self.__init_weight()

    def __init_weight(self):
        nn.init.kaiming_normal_(self.centers)



    def forward(self, x):

        features_centers_dot_product=torch.matmul(x, (self.centers))


        centers_norm=torch.sum(torch.pow(self.centers,2),0)
        centers_norm=torch.pow(centers_norm, .5)

        features_centers_dot_product=features_centers_dot_product/centers_norm
        features_centers_dot_product_square=torch.pow(features_centers_dot_product,2)

        dist=features_centers_dot_product_square+1-2*features_centers_dot_product

        # centers_square=torch.sum(torch.pow(self.centers,2),0, keepdim=True)
        # features_into_centers=2*torch.matmul(x, (self.centers))
        # dist=features_square+centers_square-features_into_centers

        return self.centers, -dist



